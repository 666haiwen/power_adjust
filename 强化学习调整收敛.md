## 强化学习调整收敛

##### v1.0(~2019.7.2)

#### 数据

**tensor**: [18,5]

18 = GeneratorsNum(8) + LoadsNum(10)

5 = [Pg, Qg, V0, Node, Type]



**state**: $Pg, Qg\in[0,10]$

**action**: 只调整发电机的有功或者无功，调整幅度为$\pm0.1$， 8 * 2 * 2 = 32

**reward**:收敛:1， 越界:-1; 其它:0



#### DQN网络

(1,16,3,3)	CNN + batchNorm

(16,32,3,3)	CNN + batchNorm

(32,32,3,3)	CNN + batchNorm

stride = 1, padding=1

==>output(18,5,32)

(, 32) fc

输入为state，输出为当前state下action对应的分数



### 训练

![../_images/reinforcement_learning_diagram.jpg](https://pytorch.org/tutorials/_images/reinforcement_learning_diagram.jpg)

**Replay Memory**:存放着2个数据池，一个是收敛的样本，一个是不收敛的样本，最开始没有收敛的样本，当开始有收敛样本之后，会在random batch中保证存在收敛的样本，两者数量都是随机的，总和为BATCH_SIZE

**Policy Net & Target Net**:都是DQN网络，Policy Net输出当前的*(state,action)*得到的*reward*，Target Net得到当前*state*下所有的*action*的*reward*，选取此时*reward*的最大值，两者对比得到loss.

$loss=Q(s,a)−(r+γmax_aQ(s′,a))$

固定步数之后，更新Targe Net = Policy Net. 逐步收敛得到真实的Q表(DQN网络)



#### 参数

```
- EPOCHS = 1000  //1000次reward = 1 / -1
- BATCH_SIZE = 512
- GAMMA = 0.999
- EPS_START = 0.9 //开始随机探索的概率
- EPS_END = 0.05  //最后随机探索的概率
- EPS_DECAY = 1000 //1000次探索之后，以0.05的概率随机探索
- TARGET_UPDATE = 10 //10个EPOCH后，更新target_net

```



#### 训练结果

不收敛样本数量：10000

收敛样本数量：250

**loss**:

##### ![1562153353084](C:\Users\chenzx\AppData\Roaming\Typora\typora-user-images\1562153353084.png)

收敛的很快，说明policy net和target net差距不大

**收敛**：

![1562153482082](C:\Users\chenzx\AppData\Roaming\Typora\typora-user-images\1562153482082.png)

120多轮过后收敛，基本上都能收敛了,后面就学偏了，越学越不收敛，700轮后loss变大，网络震荡，900轮后就又可以逐步收敛了。

